- **Current Status**: Test coverage gate temporarily lowered to 50% while rebuilding test suite with real assertions. Removed canopy-lsp crate (was entirely stubs), deleting tautological tests that always pass. Goal: honest, meaningful tests over inflated coverage numbers.

# üßπ CLEANUP COMPLETED - 2025-10-01

## Cleanup Summary

**Goal**: Streamline, simplify, and modernize the codebase for M7 progression.

### Phase 1: Critical Fixes ‚úÖ COMPLETE

- [x] **1.1**: Removed GPU code from canopy-tokenizer (deferred to M10+)
  - Deleted gpu.rs, removed wgpu/bytemuck dependencies
  - Removed GPU feature flag
- [x] **1.2**: Fixed 5 clippy collapsible_if violations
  - canopy-wordnet: 2 fixes (engine.rs lines 127, 148)
  - canopy-wordnet: 1 fix (loader.rs line 216)
  - canopy-lexicon: 2 fixes (types.rs lines 543, 577)
- [x] **1.3**: Removed 4 unused imports in canopy-treebank
  - indexer.rs, pattern_cache.rs, pattern_indexer.rs, synthesizer.rs
- [x] **1.4**: Fixed cargo-tarpaulin libgit2 dependency
  - Installed libgit2 1.9.1 via Homebrew

### Phase 2: Code Cleanup ‚úÖ COMPLETE

- [x] **2.1**: Deleted 26 disabled/backup files
  - 13 disabled examples in canopy-tokenizer
  - 7 disabled examples in canopy-pipeline
  - 1 disabled example in canopy-wordnet
  - 1 disabled benchmark in canopy-tokenizer
  - 2 .bak files in canopy-treebank
  - 2 .original files in canopy-tokenizer

### Phase 3: Documentation Updates ‚úÖ COMPLETE

- [x] Updated README.md example paths
- [x] Removed references to disabled examples
- [x] Fixed package name references

### Phase 4: Data Cleanup ‚úÖ COMPLETE

- [x] Deleted data/Ollama.dmg (saved 46MB)

### Verification Status

- [x] cargo build --workspace (SUCCESS - clean build)
- [ ] cargo clippy (PARTIAL - 16 remaining lint warnings in canopy-treebank/canopy-pipeline)
- [ ] cargo test --workspace (pending)

### Remaining Tech Debt (Low Priority)

**canopy-treebank** (14 clippy lints):

- Derive Default implementations instead of manual impl
- Simplify map_or patterns
- Use Option::map correctly
- Add Default traits where appropriate

**canopy-pipeline** (2 clippy lints):

- Remove unused variables

These are non-critical lints that don't affect functionality.

# üö® CRITICAL: NO STUB IMPLEMENTATIONS - EXTREMELY IMPORTANT üö®

## ABSOLUTE PROHIBITION ON STUB/MOCK IMPLEMENTATIONS

**STUBS ARE BANNED AND EXTREMELY NOT DESIRED. NEVER CREATE STUB IMPLEMENTATIONS.**

### What Constitutes a Stub (FORBIDDEN):

- ‚ùå Functions that return empty/placeholder data instead of real analysis
- ‚ùå Engines that claim to load data but return empty results
- ‚ùå "Graceful degradation" that falls back to meaningless placeholder responses
- ‚ùå Tests that return hardcoded fake data instead of real processing
- ‚ùå Performance benchmarks measuring empty operations instead of real work
- ‚ùå Demos that show unrealistic performance (microseconds instead of milliseconds)
- ‚ùå Any code that lies about system capabilities or performance

### REQUIRED: Real Implementation Only

- ‚úÖ **ALWAYS** load actual semantic data (VerbNet XMLs, WordNet database, FrameNet)
- ‚úÖ **ALWAYS** fail fast if data cannot be loaded - do not fall back to stubs
- ‚úÖ **ALWAYS** show realistic performance metrics (1-10ms per sentence, not microseconds)
- ‚úÖ **ALWAYS** return actual semantic analysis or clear errors
- ‚úÖ **ALWAYS** test with real data that produces meaningful results
- ‚úÖ **ALWAYS** be honest about system capabilities and limitations

### Enforcement Rules:

1. **Data Loading**: If semantic data doesn't exist, FAIL initialization - don't use stubs
1. **Performance**: Real semantic analysis takes milliseconds, not microseconds - be honest
1. **Testing**: Tests must use real data and produce meaningful results
1. **Benchmarks**: Show actual performance with real workloads, not infrastructure overhead
1. **Documentation**: Never claim capabilities that don't exist with real data

### Examples of Previously Problematic Code:

```rust
// ‚ùå FORBIDDEN - This is a stub that lies about performance
fn analyze_word(&self, word: &str) -> Result<Analysis> {
    Ok(Analysis::empty()) // This is dishonest!
}

// ‚úÖ REQUIRED - Real implementation or failure
fn analyze_word(&self, word: &str) -> Result<Analysis> {
    if !self.data_loaded {
        return Err("VerbNet data not loaded - cannot perform analysis");
    }
    // Actually process the word through loaded VerbNet XML data
    self.process_verb_with_loaded_data(word)
}
```

**Any code that violates these rules must be immediately fixed to use real implementations.**

# Script Policy

## Do Not Write Scripts to /tmp/

- ‚ùå Never write scripts to `/tmp/` directory
- ‚úÖ Either run commands directly or create scripts in the project file structure
- ‚úÖ Use the project's `scripts/` directory for utility scripts

# CRITICAL COVERAGE REQUIREMENTS - DO NOT IGNORE

## Coverage Gates for Releases

- **CURRENT GATE**: 50% (temporarily lowered while rebuilding test suite)
- **REASON**: Removing tautological tests (assert!(true), is_ok() || is_err()) that inflate coverage but test nothing
- **GOAL**: Rebuild with meaningful tests that actually verify behavior
- **M3 REQUIREMENT**: 70% minimum with real assertions only
- **M4 REQUIREMENT**: 80% minimum + clippy tech debt resolution

## What Makes a Good Test (NOT these)

- ‚ùå `assert!(true)` - Always passes, tests nothing
- ‚ùå `assert!(result.is_ok() || result.is_err())` - Tautology, always true
- ‚ùå `assert!(vec.len() >= 0)` - Unsigned can never be negative
- ‚ùå Tests that exercise stubs and verify empty returns

## Current Coverage Achievements

- **VerbNet integration**: 99.7% success rate (332/333 XML files)
- **Lemmatization integration**: 10 comprehensive integration tests with 100% accuracy

# M5 LEMMATIZATION IMPLEMENTATION - COMPLETED ‚úÖ

## Lemmatization Architecture Implementation

### COMPLETED Features:

1. **‚úÖ Lemmatizer Trait Architecture**

   - `Lemmatizer` trait with confidence scoring
   - `SimpleLemmatizer` with rule-based processing
   - `NLPRuleLemmatizer` with nlprule integration (optional feature)
   - `LemmatizerFactory` for creating appropriate lemmatizer instances

1. **‚úÖ SemanticCoordinator Integration**

   - Lemmatization preprocessing in analysis pipeline
   - Cache keys based on lemmatized forms for better hit rates
   - Graceful fallback when lemmatization fails
   - Batch processing with lemmatization support

1. **‚úÖ Performance Optimizations**

   - 54.4% cache hit rate improvement with lemmatization
   - Batch processing performs better with lemmatization (-51.7% overhead)
   - Memory efficient: \<0.5MB usage (0.5% of budget)
   - 100% lemmatization accuracy on test cases

1. **‚úÖ Comprehensive Testing**

   - 10 integration tests covering all lemmatization scenarios
   - Performance benchmarking with detailed metrics
   - Accuracy verification with confidence scoring
   - Cache effectiveness validation

## Performance Benchmarks (Release Mode)

### Single Word Analysis:

- **Without lemmatization**: 53.7Œºs per word (18,626 words/sec)
- **With lemmatization**: 85.4Œºs per word (11,703 words/sec)
- **Overhead**: 59.2% (acceptable for improved semantic accuracy)

### Batch Processing:

- **Batch overhead**: -51.7% (lemmatization IMPROVES batch performance due to caching)
- **Cache hit rate**: 54.4% (increases with usage)
- **Memory efficiency**: 0.5MB (0.5% of budget)

### Quality Metrics:

- **Lemmatization accuracy**: 100% on test cases
- **Confidence scoring**: Irregular verbs 95%, regular rules 80%, unchanged 60%
- **Fallback reliability**: Graceful degradation when engines fail

## Implementation Details

### Files Modified/Created:

- **`crates/canopy-semantic-layer/src/lemmatizer.rs`**: Complete lemmatization module
- **`crates/canopy-semantic-layer/src/coordinator.rs`**: Integration with SemanticCoordinator
- **`crates/canopy-semantic-layer/tests/lemmatization_integration_tests.rs`**: Comprehensive tests
- **`crates/canopy-semantic-layer/examples/lemmatization_benchmark.rs`**: Performance benchmark

### Configuration Options:

```rust
pub struct CoordinatorConfig {
    pub enable_lemmatization: bool,          // Default: true
    pub use_advanced_lemmatization: bool,    // Default: false (simple)
    // ... existing config options
}
```

### Usage Example:

```rust
let coordinator = SemanticCoordinator::new(CoordinatorConfig::default())?;
let result = coordinator.analyze("running")?;
// result.original_word = "running"
// result.lemma = "run"
// result.lemmatization_confidence = Some(0.8)
```

## Quality Assurance Results

### Test Coverage:

- **Unit tests**: 6 lemmatizer module tests (100% passing)
- **Integration tests**: 10 coordinator integration tests (100% passing)
- **Performance tests**: Benchmark suite with detailed metrics
- **Accuracy tests**: 100% accuracy on representative test cases

### Error Handling:

- Graceful degradation when lemmatization fails
- Fallback to simple lemmatizer when advanced features unavailable
- Proper error propagation in coordinator pipeline
- Cache invalidation on lemmatization failures

## Next Steps for M6

### Future Enhancements (Optional):

1. **Advanced NLP Rule Integration**

   - Full nlprule feature integration with proper morphological analysis
   - Context-aware lemmatization based on POS tags
   - Multilingual lemmatization support

1. **Performance Optimizations**

   - Lemmatizer warm-up with common words
   - Predictive caching based on morphological patterns
   - SIMD-accelerated string processing for rules

1. **Quality Improvements**

   - Machine learning-based confidence calibration
   - Corpus-based irregular verb discovery
   - Domain-specific lemmatization rules

## Summary

The lemmatization implementation is **COMPLETE and PRODUCTION-READY** with:

- 100% test coverage for new functionality
- Excellent performance characteristics (54.4% cache hit improvement)
- 100% accuracy on test cases with confidence scoring
- Graceful error handling and fallback strategies
- Comprehensive benchmarking and validation

This implementation provides the semantic analysis foundation for improved cache efficiency and analysis accuracy while maintaining production performance requirements.

## M4 CLIPPY TECH DEBT REQUIREMENTS

**All clippy allows must be resolved for M4 release:**

### Current Tech Debt (to be fixed by M4):

- `#![allow(clippy::uninlined_format_args)]` - Convert to modern format syntax
- `#![allow(clippy::needless_borrow)]` - Remove unnecessary explicit borrows
- `#![allow(clippy::field_reassign_with_default)]` - Use struct initialization syntax
- `#![allow(clippy::collapsible_if)]` - Simplify nested conditionals
- `#![allow(clippy::useless_vec)]` - Use arrays where appropriate
- `#![allow(clippy::manual_clamp)]` - Use `.clamp()` method
- `#![allow(clippy::enum_variant_names)]` - Improve enum naming
- `#![allow(clippy::needless_range_loop)]` - Use iterators instead of indexing
- `#![allow(clippy::new_without_default)]` - Add Default implementations
- `#![allow(clippy::clone_on_copy)]` - Remove unnecessary clones

### üö® CRITICAL: VERIFICATION REQUIREMENTS - EXTREMELY IMPORTANT üö®

### **ABSOLUTE PROHIBITION ON UNVERIFIED CLAIMS**

**‚ùå NEVER CLAIM SOMETHING WORKS WITHOUT ACTUALLY TESTING IT ‚ùå**

### What Constitutes Unverified Claims (FORBIDDEN):

- ‚ùå Saying "this will work" or "this should work" without running the code
- ‚ùå Claiming "COMPLETED" or "SUCCESS" without running actual verification commands
- ‚ùå Declaring performance improvements without measuring them
- ‚ùå Stating compilation works without running `cargo check` or `cargo build`
- ‚ùå Claiming tests pass without running them
- ‚ùå Saying coverage requirements are met without running coverage checks
- ‚ùå Making assertions about system behavior without executing the code

### REQUIRED: Test Everything Before Claims

- ‚úÖ **ALWAYS** compile and run code before claiming it works
- ‚úÖ **ALWAYS** run `cargo check` and `cargo build` before saying compilation succeeds
- ‚úÖ **ALWAYS** run tests before claiming they pass
- ‚úÖ **ALWAYS** execute coverage checks before claiming coverage requirements are met
- ‚úÖ **ALWAYS** run the actual program/demo before claiming it functions correctly
- ‚úÖ **ALWAYS** measure performance before claiming improvements
- ‚úÖ **ALWAYS** verify all claims with concrete evidence

### Verification Workflow:

1. **Implementation**: Write the code changes
1. **Compilation Check**: Run `cargo check` and verify no errors
1. **Build Check**: Run `cargo build` and verify success
1. **Test Execution**: Run relevant tests and verify they pass
1. **Coverage Verification**: Run `scripts/check-coverage.sh` and verify it passes
1. **Demo/Program Testing**: Actually run the program and verify expected behavior
1. **Performance Measurement**: If claiming performance gains, measure and show actual results
1. **Documentation**: Only then document what was **actually verified to work**

### Examples of Previously Problematic Claims:

```
‚ùå FORBIDDEN: "The caching improvements should increase hit rate to 40-70%"
‚úÖ REQUIRED: "After running the demo, cache hit rate increased from 0.3% to 43.2%"

‚ùå FORBIDDEN: "This fixes the compilation errors"
‚úÖ REQUIRED: "Ran `cargo check` - no compilation errors found"

‚ùå FORBIDDEN: "The demo runs successfully now"
‚úÖ REQUIRED: "Executed `cargo run --example basic_demo` - completed successfully with output: [paste actual output]"
```

### Release Requirements:

1. ‚úÖ Always run `scripts/check-coverage.sh` and verify it PASSES before declaring completion
1. ‚úÖ Always run `cargo build` for entire workspace and verify no errors
1. ‚úÖ Always run relevant tests and verify they pass
1. ‚úÖ Always execute the actual program/demo and verify expected behavior
1. ‚úÖ Always measure and report actual performance metrics when claiming improvements
1. ‚ùå NEVER modify coverage thresholds to make releases easier
1. ‚ùå NEVER skip verification steps to save time
1. ‚ùå NEVER make claims based on theoretical analysis alone

**VERIFICATION SAVES TIME AND PREVENTS BUGS - UNVERIFIED CLAIMS WASTE TIME AND CREATE PROBLEMS**

______________________________________________________________________

THE BELOW IS A NEW SET OF INSTRUCTIONS TO BE REVIEWED, INTEGRATED, AND CLEAND UP.

# UD TREEBANK PATTERN MATCHING PLAN FOR LAYER 1 EXTENSION

## Overview

Extend Layer 1 semantic analysis with lightweight dependency pattern matching using UD English-EWT treebank. This will add basic syntactic structure without requiring a full parser, using semantic signatures from our existing engines to match against treebank patterns.

## Architecture Design

### Core Concept

```text
Layer 1 Semantic Analysis ‚Üí Semantic Signature ‚Üí Treebank Pattern Match ‚Üí Dependency Structure
         ‚Üì                          ‚Üì                      ‚Üì                    ‚Üì
[VerbNet + FrameNet + WordNet] [Hash key]  [UD_English-EWT patterns]  [Lightweight deps]
```

### Key Components to Add

```rust
// New module: crates/canopy-semantic-layer/src/treebank_matcher.rs

pub struct TreebankMatcher {
    // Core patterns from high-frequency verbs (~500KB memory)
    core_patterns: HashMap<SemanticSignature, DependencyPattern>,

    // Adaptive cache that grows based on usage (~1MB limit)
    adaptive_cache: LruCache<SemanticSignature, DependencyPattern>,

    // Lazy-loaded treebank index (not patterns themselves)
    treebank_index: TreebankIndex,

    // Pattern synthesis for unseen cases
    pattern_synthesizer: PatternSynthesizer,
}

pub struct SemanticSignature {
    lemma: String,
    verbnet_class: Option<String>,
    framenet_frame: Option<String>,
    theta_roles: Vec<ThetaRole>,
}

pub struct DependencyPattern {
    verb_lemma: String,
    dependencies: Vec<(DepRel, String)>,  // e.g., [(nsubj, "agent"), (obj, "patient")]
    confidence: f32,
    frequency: u32,
}
```

## Implementation Phases

### Phase 1: Parse UD Treebank and Build Index (Week 1)

#### Step 1.1: Create CoNLL-U Parser

```rust
// New file: crates/canopy-semantic-layer/src/conllu.rs

pub struct ConlluReader {
    // Parse UD_English-EWT format
}

pub fn load_treebank(path: &str) -> Result<Vec<ParsedSentence>, Error> {
    // Load from data/UD_English-EWT/en_ewt-ud-train.conllu
    // Parse ~16,000 sentences
}
```

#### Step 1.2: Build Semantic Index

```rust
// Process treebank through Layer 1 to create signature‚Üípattern mappings

pub fn index_treebank(treebank_path: &str) -> Result<TreebankIndex, Error> {
    let analyzer = create_l1_analyzer()?;
    let sentences = load_treebank(treebank_path)?;
    let mut index = TreebankIndex::new();

    for sentence in sentences {
        // Run through existing Layer 1
        let semantic_result = analyzer.analyze(&sentence.text)?;

        // Extract dependency pattern from treebank
        let dep_pattern = extract_dependencies(&sentence);

        // Create mapping
        let signature = create_signature(&semantic_result);
        index.add_pattern(signature, dep_pattern);
    }

    // Save index for fast loading
    index.save_to_disk("data/cache/treebank_index.bin")?;
    Ok(index)
}
```

### Phase 2: Implement Adaptive Caching (Week 1-2)

#### Step 2.1: Core Pattern Extraction

```rust
// Extract top 500 most frequent patterns for core cache

pub fn extract_core_patterns(index: &TreebankIndex) -> HashMap<SemanticSignature, DependencyPattern> {
    // Sort by frequency
    // Take top 500 patterns (~500KB memory)
    // These cover ~70% of common sentences
}
```

#### Step 2.2: Adaptive Cache Implementation

```rust
impl AdaptiveCache {
    pub fn get_or_load(&mut self, signature: &SemanticSignature) -> Option<DependencyPattern> {
        // 1. Check core patterns (instant)
        // 2. Check adaptive cache (fast)
        // 3. Load from index if exists (slower, rare)
        // 4. Track usage for cache promotion
    }

    pub fn should_promote(&self, signature: &SemanticSignature) -> bool {
        // Promote to cache if used >N times
        self.usage_count.get(signature).map_or(false, |&count| count > 3)
    }
}
```

### Phase 3: Pattern Synthesis for Unknown Cases (Week 2)

#### Step 3.1: VerbNet to Dependency Rules

```rust
// Use VerbNet frames to synthesize dependency patterns

pub fn verbnet_to_dependencies(verbnet_class: &str) -> DependencyPattern {
    match verbnet_class {
        "give-13.1" => DependencyPattern {
            dependencies: vec![
                (DepRel::Nsubj, "agent"),
                (DepRel::Obj, "theme"),
                (DepRel::Iobj, "recipient"),
            ],
            confidence: 0.8,
        },
        // ~200 VerbNet classes map to common patterns
    }
}
```

#### Step 3.2: Fallback Pattern Generator

```rust
impl PatternSynthesizer {
    pub fn synthesize(&self, layer1: &Layer1SemanticResult) -> DependencyPattern {
        // 1. Try VerbNet mapping
        // 2. Try FrameNet frame elements
        // 3. Use positional heuristics
        // 4. Default to basic SVO pattern
    }
}
```

### Phase 4: Integration with Layer 1 (Week 2-3)

#### Step 4.1: Extend Layer1SemanticResult

```rust
// Modify existing structure
pub struct Layer1SemanticResult {
    // ... existing fields ...

    // NEW: Optional dependency structure
    pub dependencies: Option<DependencyPattern>,
    pub dependency_confidence: f32,
}
```

#### Step 4.2: Update SemanticCoordinator

```rust
impl SemanticCoordinator {
    pub fn analyze_with_dependencies(&mut self, text: &str) -> Result<Layer1SemanticResult> {
        // Existing semantic analysis
        let mut result = self.analyze(text)?;

        // NEW: Add dependency pattern matching
        if self.config.enable_dependency_matching {
            let signature = create_signature(&result);
            if let Some(pattern) = self.treebank_matcher.get_pattern(&signature) {
                result.dependencies = Some(pattern);
            }
        }

        Ok(result)
    }
}
```

## Resource Requirements

### Memory Budget

```text
Core patterns (500):        ~500KB
Adaptive cache (variable):  ~500KB-1MB
Treebank index (on disk):   ~5MB (memory-mapped as needed)
Pattern synthesizer:        ~100KB
-----------------------------------
Total RAM usage:            1-2MB additional
```

### Treebank Files

```text
Location: data/UD_English-EWT/
Files to use:
- en_ewt-ud-train.conllu (12,543 sentences)
- en_ewt-ud-dev.conllu   (2,001 sentences)
- en_ewt-ud-test.conllu  (2,077 sentences)
Total: ~16,600 sentences
```

### Performance Targets

```text
Cache hit latency:       <1Œºs
Pattern synthesis:       <10Œºs
Index lookup:           <100Œºs (rare)
Memory overhead:        <2MB
Coverage target:        85%+ sentences
```

## Testing Strategy

### Unit Tests

```rust
#[test]
fn test_treebank_loading() {
    let patterns = load_treebank("data/UD_English-EWT/en_ewt-ud-test.conllu")?;
    assert!(patterns.len() > 2000);
}

#[test]
fn test_pattern_matching() {
    let matcher = TreebankMatcher::new();
    let semantic = /* ... Layer 1 result ... */;
    let pattern = matcher.get_pattern(&semantic);
    assert!(pattern.confidence > 0.7);
}

#[test]
fn test_memory_budget() {
    let matcher = TreebankMatcher::with_memory_limit(2_000_000); // 2MB
    // Load patterns and verify memory usage
    assert!(matcher.memory_used() < 2_000_000);
}
```

### Integration Tests

```rust
#[test]
fn test_end_to_end_with_dependencies() {
    let coordinator = SemanticCoordinator::with_dependencies()?;
    let result = coordinator.analyze("John gave Mary a book")?;

    assert!(result.dependencies.is_some());
    let deps = result.dependencies.unwrap();
    assert!(deps.has_relation(DepRel::Nsubj));
    assert!(deps.has_relation(DepRel::Obj));
}
```

## Success Criteria

1. **Coverage**: 85%+ of sentences get dependency patterns
1. **Memory**: Total additional memory \<2MB
1. **Performance**: No significant slowdown to Layer 1 (\<10Œºs additional per analysis)
1. **Quality**: 80%+ accuracy on common constructions
1. **Integration**: Clean API that doesn't break existing Layer 1 users

## File Changes Required

### New Files

```
crates/canopy-semantic-layer/src/
‚îú‚îÄ‚îÄ treebank_matcher.rs     # Main pattern matching logic
‚îú‚îÄ‚îÄ conllu.rs               # CoNLL-U parser
‚îú‚îÄ‚îÄ dependency_pattern.rs   # Pattern types and synthesis
‚îî‚îÄ‚îÄ semantic_signature.rs   # Signature creation and hashing

crates/canopy-semantic-layer/tests/
‚îî‚îÄ‚îÄ treebank_integration_tests.rs

data/cache/
‚îî‚îÄ‚îÄ treebank_index.bin     # Preprocessed index (generated)
```

### Modified Files

```
crates/canopy-semantic-layer/src/
‚îú‚îÄ‚îÄ coordinator.rs          # Add dependency matching option
‚îú‚îÄ‚îÄ lib.rs                 # Export new types
‚îî‚îÄ‚îÄ types.rs               # Extend Layer1SemanticResult

crates/canopy-semantic-layer/
‚îî‚îÄ‚îÄ Cargo.toml             # Add conllu crate dependency
```

## Configuration

```rust
pub struct CoordinatorConfig {
    // ... existing fields ...

    // NEW: Dependency matching configuration
    pub enable_dependency_matching: bool,  // Default: false initially
    pub treebank_cache_size: usize,       // Default: 500 patterns
    pub adaptive_cache_size: usize,       // Default: 1000 patterns
    pub dependency_confidence_threshold: f32, // Default: 0.7
}
```

## Rollout Plan

1. **Week 1**: Implement basic treebank loading and indexing
1. **Week 1-2**: Add core pattern cache and adaptive caching
1. **Week 2**: Implement pattern synthesis for unseen cases
1. **Week 2-3**: Integrate with Layer 1, maintain backward compatibility
1. **Week 3**: Testing, optimization, and documentation
1. **Week 4**: Performance tuning and coverage analysis

## Dependencies to Add

```toml
[dependencies]
conllu = "0.4"  # For parsing CoNLL-U format
lru = "0.12"    # For adaptive cache
bincode = "1.3" # For index serialization
```

______________________________________________________________________

**END OF UD TREEBANK PATTERN MATCHING PLAN**
