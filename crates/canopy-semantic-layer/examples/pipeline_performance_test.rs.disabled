//! Pipeline Performance Integration Test
//!
//! This integration test measures the speed and efficiency of the semantic layer
//! against the Moby Dick corpus, testing various performance characteristics:
//!
//! 1. Cold cache performance (no cached data)
//! 2. Warm cache performance (with cached results)
//! 3. Memory usage and cache efficiency
//! 4. Scalability across different text sizes
//! 5. Throughput analysis (words per second)

use canopy_semantic_layer::{coordinator::CoordinatorConfig, SemanticCoordinator};
use std::collections::HashSet;
use std::error::Error;
use std::fs;
use std::time::{Duration, Instant};

/// Performance test configuration
#[derive(Debug, Clone)]
struct TestConfig {
    /// Cache budget in MB
    cache_budget_mb: usize,
    /// TTL in seconds  
    cache_ttl_seconds: u64,
    /// Enable parallel processing
    enable_parallel: bool,
    /// Confidence threshold for caching
    confidence_threshold: f32,
}

impl Default for TestConfig {
    fn default() -> Self {
        Self {
            cache_budget_mb: 200,
            // Removed: cache_ttl_seconds (not available in simplified config)
            enable_parallel: true,
            confidence_threshold: 0.01,
        }
    }
}

/// Test results for a single run
#[derive(Debug, Clone)]
struct TestResult {
    test_name: String,
    words_processed: usize,
    total_time: Duration,
    words_per_second: f64,
    cache_hit_rate: f64,
    memory_usage_mb: f64,
    errors_encountered: usize,
}

impl TestResult {
    fn new(
        name: String,
        words: usize,
        time: Duration,
        coordinator: &SemanticCoordinator,
        errors: usize,
    ) -> Self {
        let words_per_second = if time.as_secs_f64() > 0.0 {
            words as f64 / time.as_secs_f64()
        } else {
            0.0
        };

        let stats = coordinator.get_statistics();

        Self {
            test_name: name,
            words_processed: words,
            total_time: time,
            words_per_second,
            cache_hit_rate: stats.cache_hit_rate,
            memory_usage_mb: stats.memory_usage.estimated_usage_mb,
            errors_encountered: errors,
        }
    }
}

/// Test corpus extracted from Moby Dick
struct TestCorpus {
    /// Sample sizes (word counts)
    small: Vec<String>, // ~100 words
    medium: Vec<String>, // ~500 words
    large: Vec<String>,  // ~2000 words
    xl: Vec<String>,     // ~5000 words
}

impl TestCorpus {
    fn from_moby_dick_file() -> Result<Self, Box<dyn Error>> {
        // Try to load from the actual file path
        let text = match fs::read_to_string("data/test-corpus/mobydick.txt") {
            Ok(content) => {
                println!("üìñ Loaded Moby Dick corpus ({} characters)", content.len());
                content
            }
            Err(_) => {
                println!("üìñ Using embedded sample text");
                // Use a sample for demonstration
                "Call me Ishmael. Some years ago never mind how long precisely having little or no money in my purse, and nothing particular to interest me on shore, I thought I would sail about a little and see the watery part of the world. It is a way I have of driving off the spleen, and regulating the circulation. Whenever I find myself growing grim about the mouth; whenever it is a damp, drizzly November in my soul; whenever I find myself involuntarily pausing before coffin warehouses, and bringing up the rear of every funeral I meet; and especially whenever my hypos get such an upper hand of me, that it requires a strong moral principle to prevent me from deliberately stepping into the street, and methodically knocking people's hats off then, I account it high time to get to sea as soon as possible.".repeat(50)
            }
        };

        let words = extract_content_words(&text);

        Ok(Self {
            small: words.iter().take(100).cloned().collect(),
            medium: words.iter().take(500).cloned().collect(),
            large: words.iter().take(2000).cloned().collect(),
            xl: words.iter().take(5000).cloned().collect(),
        })
    }
}

fn main() -> Result<(), Box<dyn Error>> {
    // Initialize logging
    tracing_subscriber::fmt::init();

    println!("üöÄ Canopy Semantic Layer Performance Test");
    println!("========================================");

    // Load test corpus
    let corpus = TestCorpus::from_moby_dick_file()?;
    println!("üìä Test corpus loaded:");
    println!("   Small: {} words", corpus.small.len());
    println!("   Medium: {} words", corpus.medium.len());
    println!("   Large: {} words", corpus.large.len());
    println!("   XL: {} words", corpus.xl.len());

    // Configure test scenarios
    let test_configs = vec![
        ("Standard", TestConfig::default()),
        (
            "High-Performance",
            TestConfig {
                cache_budget_mb: 500,
                // Removed: cache_ttl_seconds (not available in simplified config)
                enable_parallel: true,
                confidence_threshold: 0.001,
            },
        ),
        (
            "Memory-Efficient",
            TestConfig {
                cache_budget_mb: 50,
                // Removed: cache_ttl_seconds (not available in simplified config)
                enable_parallel: false,
                confidence_threshold: 0.1,
            },
        ),
    ];

    let mut all_results = Vec::new();

    // Run performance tests for each configuration
    for (config_name, test_config) in test_configs {
        println!("\nüîß Testing Configuration: {}", config_name);
        println!(
            "   Cache: {}MB, TTL: {}s, Parallel: {}",
            test_config.cache_budget_mb, test_config.cache_ttl_seconds, test_config.enable_parallel
        );

        let coordinator_config = CoordinatorConfig {
            l1_cache_memory_mb: test_config.cache_budget_mb / 2,
            l2_cache_memory_mb: test_config.cache_budget_mb / 2,
            cache_ttl_seconds: test_config.cache_ttl_seconds,
            enable_parallel: test_config.enable_parallel,
            confidence_threshold: test_config.confidence_threshold,
            // Removed: enable_cache_warming (not available in simplified config)
            enable_verbnet: true,
            enable_framenet: true,
            enable_wordnet: true,
            enable_lexicon: true,
            graceful_degradation: true,
            ..CoordinatorConfig::default()
        };

        let coordinator = SemanticCoordinator::new(coordinator_config)?;

        // Test each corpus size
        let test_cases = vec![
            ("Small", &corpus.small),
            ("Medium", &corpus.medium),
            ("Large", &corpus.large),
            ("XL", &corpus.xl),
        ];

        for (size_name, words) in test_cases {
            // === Cold Cache Test ===
            let result = run_performance_test(
                &coordinator,
                words,
                &format!("{}-{}-Cold", config_name, size_name),
                false, // Don't warm cache
            )?;
            all_results.push(result);

            // === Warm Cache Test ===
            let result = run_performance_test(
                &coordinator,
                words,
                &format!("{}-{}-Warm", config_name, size_name),
                true, // Use warmed cache
            )?;
            all_results.push(result);
        }
    }

    // === Performance Analysis ===
    println!("\nüìà Performance Analysis");
    println!("=======================");

    analyze_results(&all_results);

    // === Cache Efficiency Analysis ===
    println!("\nüíæ Cache Efficiency Analysis");
    println!("============================");

    analyze_cache_efficiency(&all_results);

    // === Scalability Analysis ===
    println!("\nüìè Scalability Analysis");
    println!("=======================");

    analyze_scalability(&all_results);

    // === Recommendations ===
    println!("\nüí° Performance Recommendations");
    println!("==============================");

    generate_recommendations(&all_results);

    println!("\nüéØ Performance test complete!");

    Ok(())
}

/// Run a single performance test
fn run_performance_test(
    coordinator: &SemanticCoordinator,
    words: &[String],
    test_name: &str,
    warm_cache: bool,
) -> Result<TestResult, Box<dyn Error>> {
    if warm_cache {
        // Warm the cache with a subset of words
        let warmup_words: Vec<&str> = words.iter().take(50).map(|s| s.as_str()).collect();
        let _ = coordinator.warm_cache(&warmup_words);
    }

    println!("   üîç Running: {}", test_name);

    let start = Instant::now();
    let mut errors = 0;
    let mut processed = 0;

    for word in words {
        match coordinator.analyze(word) {
            Ok(_) => processed += 1,
            Err(_) => errors += 1,
        }
    }

    let duration = start.elapsed();
    let result = TestResult::new(
        test_name.to_string(),
        processed,
        duration,
        coordinator,
        errors,
    );

    println!(
        "      ‚ö° {:.0} words/sec, {:.1}% cache hits, {:.1}MB memory",
        result.words_per_second,
        result.cache_hit_rate * 100.0,
        result.memory_usage_mb
    );

    Ok(result)
}

/// Analyze performance results
fn analyze_results(results: &[TestResult]) {
    println!("\nüìä Performance Summary:");
    println!("Test                           | Words/sec | Cache Hit | Memory    | Errors");
    println!("-------------------------------|-----------|-----------|-----------|-------");

    for result in results {
        println!(
            "{:30} | {:9.0} | {:8.1}% | {:8.1}MB | {:6}",
            result.test_name,
            result.words_per_second,
            result.cache_hit_rate * 100.0,
            result.memory_usage_mb,
            result.errors_encountered
        );
    }

    // Find best performers
    if let Some(fastest) = results
        .iter()
        .max_by(|a, b| a.words_per_second.partial_cmp(&b.words_per_second).unwrap())
    {
        println!(
            "\nüèÜ Fastest: {} ({:.0} words/sec)",
            fastest.test_name, fastest.words_per_second
        );
    }

    if let Some(most_efficient) = results
        .iter()
        .max_by(|a, b| a.cache_hit_rate.partial_cmp(&b.cache_hit_rate).unwrap())
    {
        println!(
            "üíæ Most Cache Efficient: {} ({:.1}% hit rate)",
            most_efficient.test_name,
            most_efficient.cache_hit_rate * 100.0
        );
    }
}

/// Analyze cache efficiency patterns
fn analyze_cache_efficiency(results: &[TestResult]) {
    let warm_cache_results: Vec<_> = results
        .iter()
        .filter(|r| r.test_name.contains("-Warm"))
        .collect();

    let cold_cache_results: Vec<_> = results
        .iter()
        .filter(|r| r.test_name.contains("-Cold"))
        .collect();

    if !warm_cache_results.is_empty() && !cold_cache_results.is_empty() {
        let avg_warm_hit_rate = warm_cache_results
            .iter()
            .map(|r| r.cache_hit_rate)
            .sum::<f64>()
            / warm_cache_results.len() as f64;

        let avg_cold_hit_rate = cold_cache_results
            .iter()
            .map(|r| r.cache_hit_rate)
            .sum::<f64>()
            / cold_cache_results.len() as f64;

        let avg_warm_speed = warm_cache_results
            .iter()
            .map(|r| r.words_per_second)
            .sum::<f64>()
            / warm_cache_results.len() as f64;

        let avg_cold_speed = cold_cache_results
            .iter()
            .map(|r| r.words_per_second)
            .sum::<f64>()
            / cold_cache_results.len() as f64;

        println!(
            "   üî• Warm Cache: {:.1}% hit rate, {:.0} words/sec",
            avg_warm_hit_rate * 100.0,
            avg_warm_speed
        );
        println!(
            "   ‚ùÑÔ∏è  Cold Cache: {:.1}% hit rate, {:.0} words/sec",
            avg_cold_hit_rate * 100.0,
            avg_cold_speed
        );

        let speedup = avg_warm_speed / avg_cold_speed;
        println!("   üìà Cache Speedup: {:.1}x faster", speedup);
    }
}

/// Analyze scalability patterns
fn analyze_scalability(results: &[TestResult]) {
    // Group by configuration
    let mut config_groups: std::collections::HashMap<String, Vec<&TestResult>> =
        std::collections::HashMap::new();

    for result in results {
        let config = result
            .test_name
            .split('-')
            .next()
            .unwrap_or("Unknown")
            .to_string();
        config_groups.entry(config).or_default().push(result);
    }

    for (config, group) in config_groups {
        let mut sorted_group = group;
        sorted_group.sort_by_key(|r| r.words_processed);

        if sorted_group.len() >= 2 {
            let smallest = sorted_group.first().unwrap();
            let largest = sorted_group.last().unwrap();

            let size_ratio = largest.words_processed as f64 / smallest.words_processed as f64;
            let speed_ratio = largest.words_per_second / smallest.words_per_second;

            println!(
                "   {} scalability: {:.1}x size ‚Üí {:.2}x speed",
                config, size_ratio, speed_ratio
            );

            if speed_ratio > 0.8 {
                println!("      ‚úÖ Good linear scaling");
            } else if speed_ratio > 0.5 {
                println!("      ‚ö†Ô∏è  Moderate scaling degradation");
            } else {
                println!("      ‚ùå Poor scaling - needs optimization");
            }
        }
    }
}

/// Generate performance recommendations
fn generate_recommendations(results: &[TestResult]) {
    let avg_hit_rate = results.iter().map(|r| r.cache_hit_rate).sum::<f64>() / results.len() as f64;
    let avg_speed = results.iter().map(|r| r.words_per_second).sum::<f64>() / results.len() as f64;
    let max_memory = results
        .iter()
        .map(|r| r.memory_usage_mb)
        .fold(0.0, f64::max);

    println!("   üìä Overall Performance:");
    println!("      Average speed: {:.0} words/sec", avg_speed);
    println!("      Average cache hit rate: {:.1}%", avg_hit_rate * 100.0);
    println!("      Peak memory usage: {:.1}MB", max_memory);

    println!("\n   üí° Recommendations:");

    if avg_hit_rate < 0.3 {
        println!("      ‚Ä¢ Increase cache size - low hit rate detected");
    }

    if avg_speed < 1000.0 {
        println!("      ‚Ä¢ Consider enabling parallel processing for better throughput");
    }

    if max_memory > 400.0 {
        println!("      ‚Ä¢ Monitor memory usage - approaching high levels");
    }

    if avg_hit_rate > 0.8 && avg_speed > 5000.0 {
        println!("      ‚úÖ Excellent performance! Current configuration is optimal.");
    }
}

/// Extract meaningful content words from text
fn extract_content_words(text: &str) -> Vec<String> {
    let stop_words: HashSet<&str> = [
        "the", "and", "is", "was", "are", "were", "a", "an", "as", "at", "be", "by", "for", "from",
        "in", "of", "on", "to", "with", "that", "this", "it", "he", "she", "they", "we", "you",
        "i", "me", "my", "his", "her", "their", "our", "your", "but", "or", "not", "no", "yes",
        "can", "will", "would", "could", "should", "may", "might", "must", "shall", "has", "have",
        "had", "do", "does", "did",
    ]
    .iter()
    .cloned()
    .collect();

    text.split_whitespace()
        .map(|word| {
            word.chars()
                .filter(|c| c.is_alphabetic())
                .collect::<String>()
                .to_lowercase()
        })
        .filter(|word| {
            word.len() > 2
                && word.len() < 20
                && !stop_words.contains(word.as_str())
                && word.chars().all(|c| c.is_ascii_alphabetic())
        })
        .collect()
}
