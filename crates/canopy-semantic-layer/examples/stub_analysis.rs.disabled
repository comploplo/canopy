//! Stub Analysis - Revealing What's Really Happening
//!
//! This example analyzes exactly what the current semantic engines are doing
//! to demonstrate that we're using stubs, not real parsers.

use canopy_semantic_layer::{coordinator::CoordinatorConfig, SemanticCoordinator};
use std::error::Error;

fn main() -> Result<(), Box<dyn Error>> {
    println!("ğŸ•µï¸ Semantic Engine Stub Analysis");
    println!("=================================");

    let config = CoordinatorConfig {
        confidence_threshold: 0.01,
        enable_verbnet: true,
        enable_framenet: true,
        enable_wordnet: true,
        enable_lexicon: true,
        ..CoordinatorConfig::default()
    };

    let coordinator = SemanticCoordinator::new(config)?;

    println!("\nğŸ“‹ Testing Stub Behavior:");
    println!("=========================");

    // Test words that should definitely have semantic data if real parsers were working
    let test_cases = vec![
        // Words in the hardcoded test data
        ("run", "Should find VerbNet data (hardcoded)"),
        ("love", "Should find VerbNet + FrameNet data (hardcoded)"),
        // Words NOT in hardcoded data but common verbs
        ("walk", "Real parser would find this, stub won't"),
        ("eat", "Real parser would find this, stub won't"),
        ("swim", "Real parser would find this, stub won't"),
        ("think", "Real parser would find this, stub won't"),
        // Very rare/nonsense words
        ("flibbertigibbet", "Neither should find this"),
        ("xyzzy", "Neither should find this"),
    ];

    println!("\nWord Analysis:");
    for (word, expectation) in test_cases {
        println!("\nğŸ” Testing: \"{}\"", word);
        println!("   Expected: {}", expectation);

        match coordinator.analyze(word) {
            Ok(result) => {
                println!("   Result: {} sources found", result.sources.len());
                for source in &result.sources {
                    println!("     â€¢ {}", source);
                }

                // Analyze the results
                if word == "run" || word == "love" {
                    if !result.sources.is_empty() {
                        println!("   âœ… Expected result (hardcoded test data)");
                    } else {
                        println!("   âŒ Unexpected: should have found hardcoded data");
                    }
                } else if result.sources.contains(&"VerbNet".to_string())
                    || result.sources.contains(&"FrameNet".to_string())
                {
                    println!(
                        "   ğŸš¨ SUSPICIOUS: Found data for '{}' - suggests limited hardcoded data",
                        word
                    );
                } else {
                    println!("   âœ… Expected: No semantic data found (not in hardcoded set)");
                }
            }
            Err(e) => {
                println!("   Error: {}", e);
            }
        }
    }

    // Test cache behavior with repeated queries
    println!("\nğŸ”„ Cache Behavior Analysis:");
    println!("===========================");

    let repeated_word = "run";
    println!(
        "Testing repeated queries for '{}' to observe cache behavior:",
        repeated_word
    );

    for i in 1..=5 {
        let start = std::time::Instant::now();
        let result = coordinator.analyze(repeated_word)?;
        let duration = start.elapsed();

        println!(
            "   Query {}: {:.0}Î¼s, {} sources",
            i,
            duration.as_micros(),
            result.sources.len()
        );
    }

    let stats = coordinator.get_statistics();
    println!("\nğŸ“Š Final Statistics:");
    println!("   Total queries: {}", stats.total_queries);
    println!("   Cache hits: {}", stats.cache_hits);
    println!("   Cache hit rate: {:.1}%", stats.cache_hit_rate * 100.0);
    println!("   Active engines: {:?}", stats.active_engines);

    // Reveal the truth about data sources
    println!("\nğŸ” Data Source Analysis:");
    println!("========================");

    println!("The engines report as active: {:?}", stats.active_engines);
    println!("But let's check what data they actually contain...");

    println!("\nğŸ’¡ Conclusions:");
    println!("===============");

    println!("1. ğŸ­ PERFORMANCE IS MISLEADING:");
    println!("   â€¢ 400K+ words/sec is possible because we're doing hash table lookups");
    println!("   â€¢ Not parsing complex XML semantic databases");
    println!("   â€¢ Not performing deep semantic analysis");

    println!("\n2. ğŸ“Š DATA COMPLETENESS:");
    println!("   â€¢ VerbNet: Only 'run' and 'love' verb classes (2 out of 3000+)");
    println!("   â€¢ FrameNet: Only 'Motion' and 'Emotion' frames (2 out of 1200+)");
    println!("   â€¢ WordNet: Falls back to empty when real DB unavailable");
    println!("   â€¢ Lexicon: Only real engine with actual data (258 words)");

    println!("\n3. ğŸš¨ WHAT THIS MEANS:");
    println!("   â€¢ Current 'performance test' measures stub lookup speed");
    println!("   â€¢ Real semantic analysis would be 10-100x slower");
    println!("   â€¢ Cache effectiveness is on trivial data");
    println!("   â€¢ Results are not semantically meaningful");

    println!("\n4. âœ… TO GET REAL NUMBERS:");
    println!("   â€¢ Need to load actual VerbNet XML files (3000+ verb classes)");
    println!("   â€¢ Need to load actual FrameNet XML files (1200+ frames)");
    println!("   â€¢ Need to connect to real WordNet database");
    println!("   â€¢ Performance will drop dramatically but results will be meaningful");

    Ok(())
}
