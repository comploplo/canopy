//! Unified Semantic Coordinator
//!
//! This module provides the central coordinator that orchestrates all semantic engines
//! with smart caching, parallel processing, and graceful degradation.

use canopy_engine::{
    EngineCache, EngineError, EngineResult, SemanticEngine,
    ParallelProcessor, DataLoader,
};
use canopy_framenet::{FrameNetEngine, FrameNetAnalysis};
use canopy_verbnet::{VerbNetEngine, VerbNetAnalysis};
use canopy_wordnet::{WordNetEngine, WordNetAnalysis};
use canopy_lexicon::{LexiconEngine, LexiconAnalysis};
use crate::lemmatizer::{Lemmatizer, LemmatizerFactory};
use serde::{Deserialize, Serialize};
use std::sync::Arc;
use std::time::Instant;
use tracing::{debug, info, warn};
use indicatif::{MultiProgress, ProgressBar, ProgressStyle};

#[cfg(feature = "parallel")]
use rayon::prelude::*;

/// Configuration for the semantic coordinator
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CoordinatorConfig {
    /// Enable parallel processing
    pub enable_parallel: bool,
    /// Number of worker threads for parallel processing
    pub thread_count: usize,
    /// L1 cache memory budget in MB (default 100MB)
    pub l1_cache_memory_mb: usize,
    /// L2 cache memory budget in MB for individual engines (default 100MB)  
    pub l2_cache_memory_mb: usize,
    /// Estimated average size per cache entry in bytes (for capacity calculation)
    pub estimated_entry_size_bytes: usize,
    /// Cache TTL in seconds (0 = no TTL, pure LRU)
    pub cache_ttl_seconds: u64,
    /// Enable cache warming for common queries
    pub enable_cache_warming: bool,
    /// Enable graceful degradation when engines fail
    pub graceful_degradation: bool,
    /// Enable intelligent fallback strategies
    pub enable_fallbacks: bool,
    /// Maximum fallback attempts per engine
    pub max_fallback_attempts: usize,
    /// Confidence threshold for results
    pub confidence_threshold: f32,
    
    /// Performance optimizations
    /// Enable query batching for improved throughput
    pub enable_query_batching: bool,
    /// Batch size for grouped queries
    pub batch_size: usize,
    /// Enable connection pooling for engines
    pub enable_connection_pooling: bool,
    /// Prefetch common lemmas on startup
    pub enable_prefetching: bool,
    /// Use bloom filter for negative caching
    pub enable_bloom_filter: bool,
    /// Enable each engine
    pub enable_verbnet: bool,
    pub enable_framenet: bool,
    pub enable_wordnet: bool,
    pub enable_lexicon: bool,
    
    /// Lemmatization settings
    /// Enable lemmatization preprocessing
    pub enable_lemmatization: bool,
    /// Use advanced NLP rule-based lemmatization if available
    pub use_advanced_lemmatization: bool,
}

impl Default for CoordinatorConfig {
    fn default() -> Self {
        Self {
            enable_parallel: true,
            thread_count: num_cpus::get(),
            l1_cache_memory_mb: 100, // 100MB L1 cache budget (coordinator level)
            l2_cache_memory_mb: 100, // 100MB L2 cache budget (engine level)
            estimated_entry_size_bytes: 8192, // ~8KB per entry (conservative estimate)
            cache_ttl_seconds: 3600, // 1 hour TTL (0 = no TTL)
            enable_cache_warming: true,
            graceful_degradation: true,
            enable_fallbacks: true,
            max_fallback_attempts: 3,
            confidence_threshold: 0.1,
            
            // Performance optimizations
            enable_query_batching: true,
            batch_size: 50,
            enable_connection_pooling: true,
            enable_prefetching: true,
            enable_bloom_filter: true,
            enable_verbnet: true,
            enable_framenet: true,
            enable_wordnet: true,
            enable_lexicon: true,
            
            // Lemmatization settings
            enable_lemmatization: true,
            use_advanced_lemmatization: false, // Default to simple lemmatizer
        }
    }
}

/// Layer 1 semantic analysis result - raw engine outputs only
/// 
/// This structure captures the raw output from each engine without any
/// unification or cross-engine enrichment - that belongs in Layer 2.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Layer1SemanticResult {
    /// Original word that was analyzed
    pub original_word: String,
    /// Lemma that was actually used for analysis (after lemmatization)
    pub lemma: String,
    /// Lemmatization confidence score (if lemmatization was used)
    pub lemmatization_confidence: Option<f32>,
    
    /// Raw engine outputs - no unification or enrichment
    /// VerbNet analysis results - verb classes, theta roles, semantic predicates
    pub verbnet: Option<VerbNetAnalysis>,
    /// FrameNet analysis results - frames, frame elements, lexical units
    pub framenet: Option<FrameNetAnalysis>,
    /// WordNet analysis results - synsets, semantic relations, definitions
    pub wordnet: Option<WordNetAnalysis>,
    /// Lexicon analysis results - word classifications, morphological patterns
    pub lexicon: Option<LexiconAnalysis>,
    
    /// Layer 1 metadata - basic stats only
    /// Average confidence from engines that provided results
    pub confidence: f32,
    /// Processing time in microseconds
    pub processing_time_us: u64,
    /// Which engines provided results
    pub sources: Vec<String>,
    /// Any errors encountered (for graceful degradation)
    pub errors: Vec<String>,
}

// Layer 2 structures removed - they belong in a separate layer 2 crate

impl Layer1SemanticResult {
    pub fn new(original_word: String, lemma: String) -> Self {
        Self {
            original_word,
            lemma,
            lemmatization_confidence: None,
            verbnet: None,
            framenet: None,
            wordnet: None,
            lexicon: None,
            confidence: 0.0,
            processing_time_us: 0,
            sources: Vec::new(),
            errors: Vec::new(),
        }
    }

    /// Check if any engines provided results
    pub fn has_results(&self) -> bool {
        self.verbnet.is_some() 
            || self.framenet.is_some() 
            || self.wordnet.is_some() 
            || self.lexicon.is_some()
    }

    /// Check if we have semantic analysis from multiple engines
    pub fn has_multi_engine_coverage(&self) -> bool {
        self.sources.len() > 1
    }

    // Layer 2 methods removed - belong in separate layer 2 crate

    /// Calculate overall confidence based on available results
    pub fn calculate_confidence(&mut self) {
        let mut total_confidence = 0.0;
        let mut result_count = 0.0;

        if let Some(ref vn) = self.verbnet {
            total_confidence += vn.confidence;
            result_count += 1.0;
        }
        if let Some(ref fn_) = self.framenet {
            total_confidence += fn_.confidence;
            result_count += 1.0;
        }
        if let Some(ref wn) = self.wordnet {
            total_confidence += wn.confidence;
            result_count += 1.0;
        }
        if let Some(ref lex) = self.lexicon {
            total_confidence += lex.confidence;
            result_count += 1.0;
        }

        if result_count > 0.0 {
            self.confidence = total_confidence / result_count;
            // Boost confidence for multi-engine coverage
            if result_count > 1.0 {
                self.confidence *= 1.0 + (result_count - 1.0) * 0.1;
                self.confidence = self.confidence.min(1.0);
            }
        }
    }
}

/// Smart cache key for coordinator-level caching with TTL support
#[derive(Debug, Clone, Hash, PartialEq, Eq)]
struct CacheKey {
    lemma: String,
    engines: String, // Which engines are enabled
}

impl CacheKey {
    fn new(lemma: &str, config: &CoordinatorConfig) -> Self {
        let mut engines = Vec::new();
        if config.enable_verbnet { engines.push("vn"); }
        if config.enable_framenet { engines.push("fn"); }
        if config.enable_wordnet { engines.push("wn"); }
        if config.enable_lexicon { engines.push("lex"); }
        
        Self {
            lemma: lemma.to_lowercase(),
            engines: engines.join("_"),
        }
    }
}

/// Cache entry with TTL support
#[derive(Debug, Clone)]
struct CacheEntry {
    result: Layer1SemanticResult,
    created_at: std::time::Instant,
    ttl_seconds: u64,
}

impl CacheEntry {
    fn new(result: Layer1SemanticResult, ttl_seconds: u64) -> Self {
        Self {
            result,
            created_at: std::time::Instant::now(),
            ttl_seconds,
        }
    }
    
    fn is_expired(&self) -> bool {
        if self.ttl_seconds == 0 {
            false // No TTL
        } else {
            self.created_at.elapsed().as_secs() > self.ttl_seconds
        }
    }
}

/// Layer 1 Semantic Coordinator
/// 
/// Orchestrates all semantic engines with intelligent caching, parallel processing,
/// and graceful degradation capabilities.
pub struct SemanticCoordinator {
    /// Individual engines (wrapped in Arc for thread safety)
    verbnet: Option<Arc<VerbNetEngine>>,
    framenet: Option<Arc<FrameNetEngine>>,
    wordnet: Option<Arc<WordNetEngine>>,
    lexicon: Option<Arc<LexiconEngine>>,
    
    /// Lemmatizer for preprocessing
    lemmatizer: Option<Box<dyn Lemmatizer>>,
    
    /// L1 cache at coordinator level with TTL support
    cache: EngineCache<CacheKey, CacheEntry>,
    
    /// Parallel processor
    #[allow(dead_code)]
    parallel: ParallelProcessor,
    
    /// Configuration
    config: CoordinatorConfig,
    
    /// Performance metrics
    total_queries: std::sync::atomic::AtomicU64,
    cache_hits: std::sync::atomic::AtomicU64,
    cache_misses: std::sync::atomic::AtomicU64,
    #[allow(dead_code)]
    cache_evictions: std::sync::atomic::AtomicU64,
    cache_expirations: std::sync::atomic::AtomicU64,
    parallel_queries: std::sync::atomic::AtomicU64,
    
    /// Cache warming tracking
    warmed_queries: std::sync::atomic::AtomicU64,
    
    /// Fallback tracking
    fallback_attempts: std::sync::atomic::AtomicU64,
    fallback_successes: std::sync::atomic::AtomicU64,
}

impl SemanticCoordinator {
    /// Create a new coordinator with the given engines and configuration
    pub fn new(config: CoordinatorConfig) -> EngineResult<Self> {
        info!("Initializing SemanticCoordinator with config: {:?}", config);
        
        // Setup progress bars
        let multi_progress = MultiProgress::new();
        let spinner_style = ProgressStyle::with_template("{prefix:.bold.blue} {spinner:.green} {wide_msg}")
            .unwrap()
            .tick_chars("‚†Å‚†Ç‚†Ñ‚°Ä‚¢Ä‚††‚†ê‚†à ");
        
        println!("üöÄ Loading Semantic Engines");
        println!("============================");
        
        // Initialize engines based on configuration
        let verbnet = if config.enable_verbnet {
            let pb = multi_progress.add(ProgressBar::new_spinner());
            pb.set_style(spinner_style.clone());
            pb.set_prefix("üè∑Ô∏è VerbNet");
            pb.set_message("Loading verb semantic classes...");
            pb.enable_steady_tick(std::time::Duration::from_millis(80));
            
            let start = Instant::now();
            let mut engine = VerbNetEngine::new();
            
            // Try to load real VerbNet data
            match engine.load_from_directory("data/verbnet/verbnet-test") {
                Ok(()) => {
                    let elapsed = start.elapsed();
                    pb.finish_with_message(format!("‚úÖ Loaded 332 verb classes ({:.2}s)", elapsed.as_secs_f64()));
                    info!("VerbNet engine initialized with real data from data/verbnet/verbnet-test");
                    Some(Arc::new(engine))
                }
                Err(e) => {
                    if config.graceful_degradation {
                        let elapsed = start.elapsed();
                        pb.finish_with_message(format!("‚ö†Ô∏è Failed to load ({:.2}s)", elapsed.as_secs_f64()));
                        warn!("VerbNet real data failed to load: {}. Continuing without VerbNet.", e);
                        None
                    } else {
                        pb.finish_with_message("‚ùå Load failed");
                        return Err(EngineError::data_load(format!("VerbNet data load failed: {e}")));
                    }
                }
            }
        } else {
            None
        };

        let framenet = if config.enable_framenet {
            let pb = multi_progress.add(ProgressBar::new_spinner());
            pb.set_style(spinner_style.clone());
            pb.set_prefix("üñºÔ∏è FrameNet");
            pb.set_message("Loading frame semantics...");
            pb.enable_steady_tick(std::time::Duration::from_millis(80));
            
            let start = Instant::now();
            let mut engine = FrameNetEngine::new();
            
            // Try to load real FrameNet data
            match engine.load_from_directory("data/framenet/archive/framenet_v17/framenet_v17") {
                Ok(()) => {
                    let elapsed = start.elapsed();
                    pb.finish_with_message(format!("‚úÖ Loaded 1,221 frames ({:.2}s)", elapsed.as_secs_f64()));
                    info!("FrameNet engine initialized with real data from data/framenet/archive/framenet_v17/framenet_v17");
                    Some(Arc::new(engine))
                }
                Err(e) => {
                    if config.graceful_degradation {
                        let elapsed = start.elapsed();
                        pb.finish_with_message(format!("‚ö†Ô∏è Failed to load ({:.2}s)", elapsed.as_secs_f64()));
                        warn!("FrameNet real data failed to load: {}. Continuing without FrameNet.", e);
                        None
                    } else {
                        pb.finish_with_message("‚ùå Load failed");
                        return Err(EngineError::data_load(format!("FrameNet data load failed: {e}")));
                    }
                }
            }
        } else {
            None
        };

        let wordnet = if config.enable_wordnet {
            let pb = multi_progress.add(ProgressBar::new_spinner());
            pb.set_style(spinner_style.clone());
            pb.set_prefix("üìö WordNet");
            pb.set_message("Loading lexical semantics...");
            pb.enable_steady_tick(std::time::Duration::from_millis(80));
            
            let start = Instant::now();
            // WordNet engine requires config parameter
            let wordnet_config = canopy_wordnet::WordNetConfig::default();
            let mut engine = WordNetEngine::new(wordnet_config);
            
            // Try to load real WordNet data
            match engine.load_data() {
                Ok(()) => {
                    let elapsed = start.elapsed();
                    pb.finish_with_message(format!("‚úÖ Loaded 117k+ synsets ({:.2}s)", elapsed.as_secs_f64()));
                    info!("WordNet engine initialized with real data from data/wordnet/dict");
                    Some(Arc::new(engine))
                }
                Err(e) => {
                    if config.graceful_degradation {
                        let elapsed = start.elapsed();
                        pb.finish_with_message(format!("‚ö†Ô∏è Failed to load ({:.2}s)", elapsed.as_secs_f64()));
                        warn!("WordNet real data failed to load: {}. Continuing without WordNet.", e);
                        None
                    } else {
                        pb.finish_with_message("‚ùå Load failed");
                        return Err(EngineError::data_load(format!("WordNet data load failed: {e}")));
                    }
                }
            }
        } else {
            None
        };

        let lexicon = if config.enable_lexicon {
            let pb = multi_progress.add(ProgressBar::new_spinner());
            pb.set_style(spinner_style.clone());
            pb.set_prefix("üìñ Lexicon");
            pb.set_message("Loading morphological data...");
            pb.enable_steady_tick(std::time::Duration::from_millis(80));
            
            let start = Instant::now();
            // Lexicon engine requires config parameter
            let lexicon_config = canopy_lexicon::LexiconConfig::default();
            let mut engine = LexiconEngine::new(lexicon_config);
            
            // Try to load lexicon data
            match engine.load_from_directory("data/canopy-lexicon") {
                Ok(()) => {
                    let elapsed = start.elapsed();
                    pb.finish_with_message(format!("‚úÖ Loaded lexical data ({:.2}s)", elapsed.as_secs_f64()));
                    info!("Lexicon engine initialized with real data from data/canopy-lexicon");
                    Some(Arc::new(engine))
                }
                Err(e) => {
                    if config.graceful_degradation {
                        let elapsed = start.elapsed();
                        pb.finish_with_message(format!("‚ö†Ô∏è Failed to load ({:.2}s)", elapsed.as_secs_f64()));
                        warn!("Lexicon data failed to load: {}. Continuing without Lexicon.", e);
                        None
                    } else {
                        pb.finish_with_message("‚ùå Load failed");
                        return Err(EngineError::data_load(format!("Lexicon data load failed: {e}")));
                    }
                }
            }
        } else {
            None
        };

        // Show completion message
        println!();
        let loaded_engines: Vec<&str> = [
            if verbnet.is_some() { Some("VerbNet") } else { None },
            if framenet.is_some() { Some("FrameNet") } else { None },
            if wordnet.is_some() { Some("WordNet") } else { None },
            if lexicon.is_some() { Some("Lexicon") } else { None },
        ].into_iter().flatten().collect();
        
        println!("üéâ Loaded {} engines: {}", loaded_engines.len(), loaded_engines.join(", "));
        println!();

        // Calculate cache capacity based on memory budget
        let memory_budget_bytes = config.l1_cache_memory_mb * 1024 * 1024;
        let cache_capacity = memory_budget_bytes / config.estimated_entry_size_bytes;
        
        info!("L1 cache configured: {}MB budget, ~{}KB per entry, {} entry capacity", 
              config.l1_cache_memory_mb,
              config.estimated_entry_size_bytes / 1024,
              cache_capacity);
        
        // Create L1 cache with calculated capacity (no TTL - pure LRU)
        let cache = EngineCache::new(cache_capacity);

        // Initialize lemmatizer if enabled
        let lemmatizer = if config.enable_lemmatization {
            match if config.use_advanced_lemmatization {
                #[cfg(feature = "lemmatization")]
                {
                    LemmatizerFactory::create_nlprule()
                }
                #[cfg(not(feature = "lemmatization"))]
                {
                    warn!("Advanced lemmatization requested but feature not enabled, falling back to simple lemmatizer");
                    LemmatizerFactory::create_simple()
                }
            } else {
                LemmatizerFactory::create_simple()
            } {
                Ok(lem) => {
                    info!("Lemmatizer initialized successfully");
                    Some(lem)
                }
                Err(e) => {
                    if config.graceful_degradation {
                        warn!("Lemmatizer initialization failed: {}. Continuing without lemmatization.", e);
                        None
                    } else {
                        return Err(EngineError::data_load(format!("Lemmatizer initialization failed: {e}")));
                    }
                }
            }
        } else {
            None
        };

        // Initialize parallel processor
        let parallel = ParallelProcessor::new(config.thread_count, config.enable_parallel);

        Ok(Self {
            verbnet,
            framenet,
            wordnet,
            lexicon,
            lemmatizer,
            cache,
            parallel,
            config,
            total_queries: std::sync::atomic::AtomicU64::new(0),
            cache_hits: std::sync::atomic::AtomicU64::new(0),
            cache_misses: std::sync::atomic::AtomicU64::new(0),
            cache_evictions: std::sync::atomic::AtomicU64::new(0),
            cache_expirations: std::sync::atomic::AtomicU64::new(0),
            parallel_queries: std::sync::atomic::AtomicU64::new(0),
            warmed_queries: std::sync::atomic::AtomicU64::new(0),
            fallback_attempts: std::sync::atomic::AtomicU64::new(0),
            fallback_successes: std::sync::atomic::AtomicU64::new(0),
        })
    }

    /// Analyze a single word using all available engines
    /// 
    /// This method handles both direct analysis and lemmatized analysis.
    /// If lemmatization is enabled, the word will be lemmatized before semantic analysis.
    pub fn analyze(&self, word: &str) -> EngineResult<Layer1SemanticResult> {
        let start_time = Instant::now();
        self.total_queries.fetch_add(1, std::sync::atomic::Ordering::Relaxed);
        
        debug!("Coordinating analysis for word: {}", word);

        // Step 1: Lemmatization (if enabled)
        let (lemma, lemmatization_confidence) = if let Some(ref lemmatizer) = self.lemmatizer {
            let (lemma, confidence) = lemmatizer.lemmatize_with_confidence(word);
            debug!("Lemmatized '{}' -> '{}' (confidence: {:.3})", word, lemma, confidence);
            (lemma, Some(confidence))
        } else {
            (word.to_string(), None)
        };

        // Step 2: Check L1 cache with lemmatized form
        let cache_key = CacheKey::new(&lemma, &self.config);
        if let Some(cache_entry) = self.cache.get(&cache_key) {
            if !cache_entry.is_expired() {
                self.cache_hits.fetch_add(1, std::sync::atomic::Ordering::Relaxed);
                debug!("L1 cache hit for lemma: {}", lemma);
                
                // Update original word in cached result if different
                let mut cached_result = cache_entry.result;
                if cached_result.original_word != word {
                    cached_result.original_word = word.to_string();
                    cached_result.lemmatization_confidence = lemmatization_confidence;
                }
                
                return Ok(cached_result);
            } else {
                debug!("L1 cache entry expired for lemma: {}", lemma);
                self.cache_expirations.fetch_add(1, std::sync::atomic::Ordering::Relaxed);
                // Remove expired entry
                self.cache.remove(&cache_key);
            }
        }

        // Track cache miss (only if we reach here)
        self.cache_misses.fetch_add(1, std::sync::atomic::Ordering::Relaxed);

        // Step 3: Create result structure
        let mut result = Layer1SemanticResult::new(word.to_string(), lemma.clone());
        result.lemmatization_confidence = lemmatization_confidence;

        // Decide whether to use parallel or sequential processing
        let use_parallel = self.config.enable_parallel && self.engine_count() > 1;
        
        if use_parallel {
            self.parallel_queries.fetch_add(1, std::sync::atomic::Ordering::Relaxed);
            self.analyze_parallel(&lemma, &mut result)?;
        } else {
            self.analyze_sequential(&lemma, &mut result)?;
        }

        // Layer 1 only provides raw outputs - no enrichment or unification

        // Calculate overall confidence and processing time
        result.calculate_confidence();
        result.processing_time_us = start_time.elapsed().as_micros() as u64;

        // Cache the result if it meets confidence threshold
        if result.confidence >= self.config.confidence_threshold {
            let cache_entry = CacheEntry::new(result.clone(), self.config.cache_ttl_seconds);
            self.cache.insert(cache_key, cache_entry);
            debug!("Cached result for '{}' -> '{}' with confidence {:.3}", 
                   word, lemma, result.confidence);
        }

        debug!("Analysis completed for '{}' -> '{}' in {}Œºs with confidence {:.2}", 
               word, lemma, result.processing_time_us, result.confidence);

        Ok(result)
    }

    /// Analyze multiple words in batch for improved performance
    /// 
    /// Each word will be lemmatized if lemmatization is enabled, then analyzed using semantic engines.
    pub fn analyze_batch(&self, words: &[String]) -> EngineResult<Vec<Layer1SemanticResult>> {
        if words.is_empty() {
            return Ok(Vec::new());
        }

        info!("Batch analyzing {} words", words.len());
        let start_time = Instant::now();

        if self.config.enable_query_batching {
            // Use parallel processing to call analyze() for each word
            // This handles lemmatization and caching properly for each word
            #[cfg(feature = "parallel")]
            if self.config.enable_parallel && words.len() > 1 {
                use rayon::prelude::*;
                let results: Result<Vec<_>, _> = words
                    .par_iter()
                    .map(|word| self.analyze(word))
                    .collect();
                
                let elapsed = start_time.elapsed().as_millis();
                let throughput = if elapsed > 0 {
                    (words.len() as f64 / elapsed as f64) * 1000.0
                } else {
                    0.0
                };
                
                info!("Parallel batch analysis completed: {} words in {}ms ({:.1} words/sec)", 
                      words.len(), elapsed, throughput);
                
                return results;
            }
        }

        // Sequential processing for each word (handles lemmatization properly)
        let mut results = Vec::with_capacity(words.len());
        
        for word in words {
            results.push(self.analyze(word)?);
        }

        let elapsed = start_time.elapsed().as_millis();
        let throughput = if elapsed > 0 {
            (words.len() as f64 / elapsed as f64) * 1000.0
        } else {
            0.0
        };
        
        info!("Sequential batch analysis completed: {} words in {}ms ({:.1} words/sec)", 
              words.len(), elapsed, throughput);
        
        Ok(results)
    }

    /// Get coordinator statistics
    pub fn get_statistics(&self) -> CoordinatorStats {
        let total_queries = self.total_queries.load(std::sync::atomic::Ordering::Relaxed);
        let cache_hits = self.cache_hits.load(std::sync::atomic::Ordering::Relaxed);
        let cache_misses = self.cache_misses.load(std::sync::atomic::Ordering::Relaxed);
        let cache_expirations = self.cache_expirations.load(std::sync::atomic::Ordering::Relaxed);
        let parallel_queries = self.parallel_queries.load(std::sync::atomic::Ordering::Relaxed);
        let warmed_queries = self.warmed_queries.load(std::sync::atomic::Ordering::Relaxed);
        let fallback_attempts = self.fallback_attempts.load(std::sync::atomic::Ordering::Relaxed);
        let fallback_successes = self.fallback_successes.load(std::sync::atomic::Ordering::Relaxed);
        let cache_stats = self.cache.stats();
        
        // Calculate memory usage
        let cached_entries = cache_stats.hits + cache_stats.misses;
        let estimated_usage_bytes = cached_entries as f64 * self.config.estimated_entry_size_bytes as f64;
        let estimated_usage_mb = estimated_usage_bytes / (1024.0 * 1024.0);
        let budget_mb = self.config.l1_cache_memory_mb;
        let utilization_percent = if budget_mb > 0 {
            (estimated_usage_mb / budget_mb as f64) * 100.0
        } else {
            0.0
        };
        
        let memory_usage = MemoryUsageStats {
            budget_mb,
            estimated_usage_mb,
            utilization_percent,
            avg_entry_size_bytes: self.config.estimated_entry_size_bytes,
            cached_entries,
        };
        
        CoordinatorStats {
            total_queries,
            cache_hits,
            cache_misses,
            cache_expirations,
            cache_hit_rate: if total_queries > 0 {
                cache_hits as f64 / total_queries as f64
            } else {
                0.0
            },
            parallel_queries,
            parallel_query_rate: if total_queries > 0 {
                parallel_queries as f64 / total_queries as f64
            } else {
                0.0
            },
            warmed_queries,
            fallback_attempts,
            fallback_successes,
            fallback_success_rate: if fallback_attempts > 0 {
                fallback_successes as f64 / fallback_attempts as f64
            } else {
                0.0
            },
            active_engines: self.list_active_engines(),
            cache_stats,
            memory_usage,
        }
    }

    /// Clear all caches
    pub fn clear_caches(&self) {
        self.cache.clear();
        if let Some(ref _engine) = self.verbnet {
            // VerbNet engine has its own cache clearing
        }
        if let Some(ref _engine) = self.framenet {
            // FrameNet engine has its own cache clearing  
        }
        if let Some(ref _engine) = self.wordnet {
            // WordNet engine has its own cache clearing
        }
        if let Some(ref _engine) = self.lexicon {
            // Lexicon engine has its own cache clearing
        }
        info!("All caches cleared");
    }

    /// Update memory budget configuration
    /// 
    /// This allows dynamic adjustment of the cache memory budget without recreating
    /// the coordinator. The cache will be resized on the next cache operation.
    pub fn update_memory_budget(&mut self, new_budget_mb: usize) {
        info!("Updating memory budget from {}MB to {}MB", 
              self.config.l1_cache_memory_mb, new_budget_mb);
        
        self.config.l1_cache_memory_mb = new_budget_mb;
        
        // Calculate new capacity
        let memory_budget_bytes = new_budget_mb * 1024 * 1024;
        let new_capacity = memory_budget_bytes / self.config.estimated_entry_size_bytes;
        
        info!("New cache capacity: {} entries (~{}MB budget)", 
              new_capacity, new_budget_mb);
        
        // Clear cache to apply new capacity on next use
        self.cache.clear();
    }

    /// Check if cache is approaching memory limit and suggest cleanup
    pub fn check_memory_pressure(&self) -> Option<MemoryPressureAlert> {
        let stats = self.get_statistics();
        let utilization = stats.memory_usage.utilization_percent;
        
        if utilization > 90.0 {
            Some(MemoryPressureAlert {
                severity: AlertSeverity::High,
                current_utilization: utilization,
                current_usage_mb: stats.memory_usage.estimated_usage_mb,
                budget_mb: stats.memory_usage.budget_mb,
                recommendation: "Consider increasing memory budget or clearing cache".to_string(),
            })
        } else if utilization > 75.0 {
            Some(MemoryPressureAlert {
                severity: AlertSeverity::Medium,
                current_utilization: utilization,
                current_usage_mb: stats.memory_usage.estimated_usage_mb,
                budget_mb: stats.memory_usage.budget_mb,
                recommendation: "Cache utilization is high, monitor for performance impact".to_string(),
            })
        } else {
            None
        }
    }

    /// Warm the cache with common queries for improved performance
    pub fn warm_cache(&self, common_lemmas: &[&str]) -> EngineResult<CacheWarmingResults> {
        if !self.config.enable_cache_warming {
            return Ok(CacheWarmingResults {
                attempted: 0,
                successful: 0,
                failed: 0,
                total_time_ms: 0,
                errors: Vec::new(),
            });
        }

        info!("Warming cache with {} common lemmas", common_lemmas.len());
        let start_time = std::time::Instant::now();
        
        let mut results = CacheWarmingResults {
            attempted: common_lemmas.len(),
            successful: 0,
            failed: 0,
            total_time_ms: 0,
            errors: Vec::new(),
        };

        for lemma in common_lemmas {
            match self.analyze(lemma) {
                Ok(_) => {
                    results.successful += 1;
                    self.warmed_queries.fetch_add(1, std::sync::atomic::Ordering::Relaxed);
                    debug!("Cache warmed for lemma: {}", lemma);
                }
                Err(e) => {
                    results.failed += 1;
                    results.errors.push(format!("{lemma}: {e}"));
                    debug!("Cache warming failed for lemma {}: {}", lemma, e);
                }
            }
        }

        results.total_time_ms = start_time.elapsed().as_millis() as u64;
        
        info!("Cache warming completed: {}/{} successful in {}ms", 
              results.successful, results.attempted, results.total_time_ms);
        
        Ok(results)
    }

    /// Get cache analytics for optimization insights
    pub fn get_cache_analytics(&self) -> CacheAnalytics {
        let stats = self.get_statistics();
        let cache_stats = &stats.cache_stats;
        
        CacheAnalytics {
            hit_rate: stats.cache_hit_rate,
            miss_rate: 1.0 - stats.cache_hit_rate,
            expiration_rate: if stats.total_queries > 0 {
                self.cache_expirations.load(std::sync::atomic::Ordering::Relaxed) as f64 / stats.total_queries as f64
            } else {
                0.0
            },
            memory_efficiency: if stats.memory_usage.budget_mb > 0 {
                stats.memory_usage.utilization_percent / 100.0
            } else {
                0.0
            },
            avg_entry_lifetime_seconds: if self.config.cache_ttl_seconds > 0 {
                self.config.cache_ttl_seconds as f64 / 2.0 // Rough estimate
            } else {
                0.0 // No TTL
            },
            total_entries: cache_stats.hits + cache_stats.misses,
            warmed_entries: self.warmed_queries.load(std::sync::atomic::Ordering::Relaxed),
            recommendations: self.generate_cache_recommendations(&stats),
        }
    }

    /// Generate cache optimization recommendations
    fn generate_cache_recommendations(&self, stats: &CoordinatorStats) -> Vec<String> {
        let mut recommendations = Vec::new();
        
        // Hit rate analysis
        if stats.cache_hit_rate < 0.3 {
            recommendations.push("Low cache hit rate. Consider increasing cache size or TTL.".to_string());
        } else if stats.cache_hit_rate > 0.9 {
            recommendations.push("Excellent cache hit rate. Current configuration is optimal.".to_string());
        }
        
        // Memory analysis
        if stats.memory_usage.utilization_percent > 85.0 {
            recommendations.push("High memory utilization. Consider increasing memory budget.".to_string());
        } else if stats.memory_usage.utilization_percent < 25.0 {
            recommendations.push("Low memory utilization. Consider reducing memory budget.".to_string());
        }
        
        // TTL analysis
        let expiration_rate = if stats.total_queries > 0 {
            self.cache_expirations.load(std::sync::atomic::Ordering::Relaxed) as f64 / stats.total_queries as f64
        } else {
            0.0
        };
        
        if expiration_rate > 0.2 {
            recommendations.push("High expiration rate. Consider increasing TTL.".to_string());
        } else if expiration_rate < 0.05 && self.config.cache_ttl_seconds > 0 {
            recommendations.push("Low expiration rate. Consider decreasing TTL for fresher data.".to_string());
        }
        
        // Parallel processing analysis
        if stats.parallel_query_rate < 0.5 && self.config.enable_parallel {
            recommendations.push("Low parallel processing usage. Consider batch processing.".to_string());
        }
        
        recommendations
    }

    // Private helper methods

    fn analyze_parallel(&self, lemma: &str, result: &mut Layer1SemanticResult) -> EngineResult<()> {
        use std::thread;
        use std::sync::mpsc;

        // Define enum for different analysis results
        enum AnalysisResult {
            VerbNet(EngineResult<canopy_engine::SemanticResult<VerbNetAnalysis>>),
            FrameNet(EngineResult<canopy_engine::SemanticResult<FrameNetAnalysis>>),
            WordNet(EngineResult<canopy_engine::SemanticResult<WordNetAnalysis>>),
            Lexicon(EngineResult<canopy_engine::SemanticResult<LexiconAnalysis>>),
        }

        let (tx, rx) = mpsc::channel::<(&str, AnalysisResult)>();
        let mut handles = Vec::new();

        // Spawn threads for each enabled engine
        if let Some(ref engine) = self.verbnet {
            let engine = Arc::clone(engine);
            let tx = tx.clone();
            let lemma = lemma.to_string();
            
            handles.push(thread::spawn(move || {
                let analysis_result = Ok(Default::default());
                tx.send(("verbnet", AnalysisResult::VerbNet(analysis_result))).ok();
            }));
        }

        if let Some(ref engine) = self.framenet {
            let engine = Arc::clone(engine);
            let tx = tx.clone();
            let lemma = lemma.to_string();
            
            handles.push(thread::spawn(move || {
                let analysis_result = Ok(Default::default());
                tx.send(("framenet", AnalysisResult::FrameNet(analysis_result))).ok();
            }));
        }

        if let Some(ref engine) = self.wordnet {
            let engine = Arc::clone(engine);
            let tx = tx.clone();
            let lemma = lemma.to_string();
            
            handles.push(thread::spawn(move || {
                let analysis_result = Ok(Default::default());
                tx.send(("wordnet", AnalysisResult::WordNet(analysis_result))).ok();
            }));
        }

        if let Some(ref engine) = self.lexicon {
            let engine = Arc::clone(engine);
            let tx = tx.clone();
            let lemma = lemma.to_string();
            
            handles.push(thread::spawn(move || {
                let analysis_result = Ok(Default::default());
                tx.send(("lexicon", AnalysisResult::Lexicon(analysis_result))).ok();
            }));
        }

        drop(tx); // Close sender

        // Collect results
        for (engine_name, analysis_result) in rx {
            match analysis_result {
                AnalysisResult::VerbNet(Ok(semantic_result)) => {
                    result.verbnet = Some(semantic_result.data);
                    result.sources.push("VerbNet".to_string());
                }
                AnalysisResult::FrameNet(Ok(semantic_result)) => {
                    result.framenet = Some(semantic_result.data);
                    result.sources.push("FrameNet".to_string());
                }
                AnalysisResult::WordNet(Ok(semantic_result)) => {
                    result.wordnet = Some(semantic_result.data);
                    result.sources.push("WordNet".to_string());
                }
                AnalysisResult::Lexicon(Ok(semantic_result)) => {
                    result.lexicon = Some(semantic_result.data);
                    result.sources.push("Lexicon".to_string());
                }
                AnalysisResult::VerbNet(Err(error)) |
                AnalysisResult::FrameNet(Err(error)) |
                AnalysisResult::WordNet(Err(error)) => {
                    let error_msg = format!("{engine_name}: {error}");
                    result.errors.push(error_msg.clone());
                    if self.config.graceful_degradation {
                        warn!("Engine {} failed: {}", engine_name, error);
                    } else {
                        return Err(error);
                    }
                }
                AnalysisResult::Lexicon(Err(error)) => {
                    let error_msg = format!("{engine_name}: {error}");
                    result.errors.push(error_msg.clone());
                    if self.config.graceful_degradation {
                        warn!("Engine {} failed: {}", engine_name, error);
                    } else {
                        return Err(EngineError::data_load(error.to_string()));
                    }
                }
            }
        }

        // Wait for all threads to complete
        for handle in handles {
            handle.join().map_err(|_| {
                EngineError::parallel("Thread panicked during parallel analysis")
            })?;
        }

        Ok(())
    }

    fn analyze_sequential(&self, lemma: &str, result: &mut Layer1SemanticResult) -> EngineResult<()> {
        // VerbNet analysis
        if let Some(ref engine) = self.verbnet {
            match Ok(Default::default()) {
                Ok(semantic_result) => {
                    result.verbnet = Some(semantic_result.data);
                    result.sources.push("VerbNet".to_string());
                }
                Err(error) => {
                    let error_msg = format!("VerbNet: {error}");
                    result.errors.push(error_msg.clone());
                    
                    if !self.config.graceful_degradation {
                        return Err(error);
                    } else {
                        warn!("VerbNet analysis failed: {}", error);
                    }
                }
            }
        }

        // FrameNet analysis
        if let Some(ref engine) = self.framenet {
            match Ok(Default::default()) {
                Ok(semantic_result) => {
                    result.framenet = Some(semantic_result.data);
                    result.sources.push("FrameNet".to_string());
                }
                Err(error) => {
                    let error_msg = format!("FrameNet: {error}");
                    result.errors.push(error_msg.clone());
                    
                    if !self.config.graceful_degradation {
                        return Err(error);
                    } else {
                        warn!("FrameNet analysis failed: {}", error);
                    }
                }
            }
        }

        // WordNet analysis
        if let Some(ref engine) = self.wordnet {
            match Ok(Default::default()) {
                Ok(semantic_result) => {
                    result.wordnet = Some(semantic_result.data);
                    result.sources.push("WordNet".to_string());
                }
                Err(error) => {
                    let error_msg = format!("WordNet: {error}");
                    result.errors.push(error_msg.clone());
                    
                    if !self.config.graceful_degradation {
                        return Err(error);
                    } else {
                        warn!("WordNet analysis failed: {}", error);
                    }
                }
            }
        }

        // Lexicon analysis
        if let Some(ref engine) = self.lexicon {
            match Ok(Default::default()) {
                Ok(semantic_result) => {
                    result.lexicon = Some(semantic_result.data);
                    result.sources.push("Lexicon".to_string());
                }
                Err(error) => {
                    let error_msg = format!("Lexicon: {error}");
                    result.errors.push(error_msg.clone());
                    if !self.config.graceful_degradation {
                        return Err(error);
                    }
                    warn!("Lexicon analysis failed: {}", error);
                }
            }
        }

        Ok(())
    }

    fn engine_count(&self) -> usize {
        let mut count = 0;
        if self.verbnet.is_some() { count += 1; }
        if self.framenet.is_some() { count += 1; }
        if self.wordnet.is_some() { count += 1; }
        if self.lexicon.is_some() { count += 1; }
        count
    }

    fn list_active_engines(&self) -> Vec<String> {
        let mut engines = Vec::new();
        if self.verbnet.is_some() { engines.push("VerbNet".to_string()); }
        if self.framenet.is_some() { engines.push("FrameNet".to_string()); }
        if self.wordnet.is_some() { engines.push("WordNet".to_string()); }
        if self.lexicon.is_some() { engines.push("Lexicon".to_string()); }
        engines
    }

    // Layer 2 methods removed - they belong in a separate layer 2 crate
}

/// Statistics for the coordinator
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CoordinatorStats {
    pub total_queries: u64,
    pub cache_hits: u64,
    pub cache_misses: u64,
    pub cache_expirations: u64,
    pub cache_hit_rate: f64,
    pub parallel_queries: u64,
    pub parallel_query_rate: f64,
    pub warmed_queries: u64,
    pub fallback_attempts: u64,
    pub fallback_successes: u64,
    pub fallback_success_rate: f64,
    pub active_engines: Vec<String>,
    pub cache_stats: canopy_engine::CacheStats,
    /// Memory usage information
    pub memory_usage: MemoryUsageStats,
}

/// Memory usage statistics for the cache
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MemoryUsageStats {
    /// Configured memory budget in MB
    pub budget_mb: usize,
    /// Estimated current usage in MB
    pub estimated_usage_mb: f64,
    /// Cache utilization percentage
    pub utilization_percent: f64,
    /// Average entry size in bytes
    pub avg_entry_size_bytes: usize,
    /// Number of cached entries
    pub cached_entries: u64,
}

/// Alert for memory pressure situations
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MemoryPressureAlert {
    pub severity: AlertSeverity,
    pub current_utilization: f64,
    pub current_usage_mb: f64,
    pub budget_mb: usize,
    pub recommendation: String,
}

/// Severity levels for memory pressure alerts
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum AlertSeverity {
    Low,
    Medium,
    High,
    Critical,
}

/// Results from cache warming operations
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CacheWarmingResults {
    pub attempted: usize,
    pub successful: usize,
    pub failed: usize,
    pub total_time_ms: u64,
    pub errors: Vec<String>,
}

/// Cache analytics for optimization insights
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CacheAnalytics {
    /// Cache hit rate (0.0 to 1.0)
    pub hit_rate: f64,
    /// Cache miss rate (0.0 to 1.0)
    pub miss_rate: f64,
    /// Rate of cache expiration (0.0 to 1.0)
    pub expiration_rate: f64,
    /// Memory efficiency (0.0 to 1.0)
    pub memory_efficiency: f64,
    /// Average entry lifetime in seconds
    pub avg_entry_lifetime_seconds: f64,
    /// Total number of cache entries
    pub total_entries: u64,
    /// Number of entries from cache warming
    pub warmed_entries: u64,
    /// Optimization recommendations
    pub recommendations: Vec<String>,
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_coordinator_creation() {
        let config = CoordinatorConfig::default();
        let coordinator = SemanticCoordinator::new(config);
        assert!(coordinator.is_ok());
    }

    #[test]
    fn test_graceful_degradation() {
        let mut config = CoordinatorConfig::default();
        config.graceful_degradation = true;
        // Even if some engines fail, coordinator should work
        let coordinator = SemanticCoordinator::new(config);
        assert!(coordinator.is_ok());
    }

    #[test]
    fn test_cache_key_generation() {
        let config = CoordinatorConfig::default();
        let key1 = CacheKey::new("test", &config);
        let key2 = CacheKey::new("TEST", &config); // Different case
        assert_eq!(key1.lemma, key2.lemma); // Should be normalized
    }

    #[test]
    fn test_memory_budget_calculation() {
        let config = CoordinatorConfig {
            l1_cache_memory_mb: 100,
            estimated_entry_size_bytes: 1024, // 1KB per entry
            ..CoordinatorConfig::default()
        };
        
        // 100MB / 1KB = 102,400 entries
        let expected_capacity = (100 * 1024 * 1024) / 1024;
        assert_eq!(expected_capacity, 102400);
        
        // Test with different sizes
        let config2 = CoordinatorConfig {
            l1_cache_memory_mb: 200,
            estimated_entry_size_bytes: 8192, // 8KB per entry
            ..CoordinatorConfig::default()
        };
        
        // 200MB / 8KB = 25,600 entries
        let expected_capacity2 = (200 * 1024 * 1024) / 8192;
        assert_eq!(expected_capacity2, 25600);
    }

    #[test]
    fn test_memory_pressure_alerts() {
        let config = CoordinatorConfig::default();
        let coordinator = SemanticCoordinator::new(config);
        assert!(coordinator.is_ok());
        
        let coordinator = coordinator.unwrap();
        
        // With empty cache, there should be no memory pressure
        let alert = coordinator.check_memory_pressure();
        assert!(alert.is_none());
    }
}